{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Bot task"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are Online Learning platform.  \n",
    "In support chat students ask their questions, majority of which can be solved by providing link to video, where students can find answer. Often questions and answers are repeated. To automate this process we decided to create chat bot, which will help provide proper video to students questions.  \n",
    "How we can evaluate how relevant videos this chat bot sends?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have developed 2 versions of chat bot in this task  \n",
    "therefore now, we need to evaluate them and choose one"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How we can understand, which version of chat  bot if better?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Gain (CG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to understand which model is better in ranging videos, we will Summarize values of relevance in their recommendations:  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consider that we took 3 top videos by some query:  \n",
    "- 1st model had following marks of relevance: [0.99, 0.91, 0.83]  \n",
    "- 2nd model had following marks of relevance: [0.99, 0.94, 0.88]  \n",
    "\n",
    "CG@k=∑ rel, where rel - relevance of one of the items in the result, k - number of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CG1 = 2.73\n",
      "Cg2 = 2.81\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "first_marks = [0.99, 0.91, 0.83]\n",
    "second_marks = [0.99, 0.94, 0.88]\n",
    "cg1 = np.sum(first_marks)\n",
    "cg2 = np.sum(second_marks)\n",
    "print(f'CG1 = {cg1}')\n",
    "print(f'Cg2 = {cg2}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of objects in the result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@k - can be found in many metrics connected with recommendation systems.  \n",
    "Usually in real data we have hundreds or millions items (goods in online shop etc.)  \n",
    "Recommendation system can range such objects, but on practice we usually need to evaluate 1st 5 or 20 objects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function formatting:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.26"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance = [0.99, 0.94, 0.88, 0.74, 0.71, 0.68]\n",
    "k = 5\n",
    "relevance_considered = relevance[:k]\n",
    "score = np.sum(relevance_considered)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def cumulative_gain(relevance: List[float], k: int) -> float:\n",
    "    \"\"\"Score is cumulative gain at k (CG@k)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    relevance:  `List[float]`\n",
    "        Relevance labels (Ranks)\n",
    "    k : `int`\n",
    "        Number of elements to be counted\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "    \"\"\"\n",
    "    relevance_considered = relevance[:k]\n",
    "    score = np.sum(relevance_considered)\n",
    "    return score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penalty for place in the output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have got easily interpreted useful metric: we take N top recommendations, summarize their relevances and in that way we compare different models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, during the further investigation we realized that the most relevant object can be on 1st or 5th place and this would not influence our metric (in other words, we would consider 2 such models as the same in performance)  \n",
    "So how we can include Place of object in consideration?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics in Search engines:  \n",
    "Google says that 50% of users finish searching after 1st or 2nd link  \n",
    "91% of users does not go further than 1st page of the response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that if the most relevant document is in the response but does not placed on the 1st or send place, there is a high probability that user will not see it.  \n",
    "Therefore, we need to penalize models for putting highly relevant documents lower in the order"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discounted Cumulative Gain  \n",
    "We will add penalty to the relevance if important document is in the end of list or we will add weight if document is in the beginning of the list:  \n",
    "![](./pic/2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets' consider the following results of 2 models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order = 1, relevance_i = 0.99\n",
      "dcg_iter = 0.99\n",
      "order = 2, relevance_i = 0.94\n",
      "dcg_iter = 0.59307396835717\n",
      "order = 3, relevance_i = 0.88\n",
      "dcg_iter = 0.44\n",
      "dcg1 = 2.02307396835717\n",
      "dcg2 = 1.9586716954643097\n"
     ]
    }
   ],
   "source": [
    "first_marks = [0.99, 0.94, 0.88]\n",
    "second_marks = [0.99, 0.83, 0.89]\n",
    "\n",
    "dcg_array_1 = []\n",
    "dcg_array_2 = []\n",
    "for order in range(1, len(first_marks)+1):\n",
    "    i = order - 1\n",
    "    print(f'order = {order}, relevance_i = {first_marks[i]}')\n",
    "    dcg_iter = first_marks[i] / np.log2(order+1)\n",
    "    print(f'dcg_iter = {dcg_iter}')\n",
    "    dcg_array_1.append(dcg_iter)\n",
    "\n",
    "for order in range(1, len(second_marks)+1):\n",
    "    i = order - 1\n",
    "    dcg_iter = second_marks[i] / np.log2(order+1)\n",
    "    dcg_array_2.append(dcg_iter)\n",
    "\n",
    "dcg1 = np.sum(dcg_array_1)\n",
    "dcg2 = np.sum(dcg_array_2)\n",
    "print(f'dcg1 = {dcg1}')\n",
    "print(f'dcg2 = {dcg2}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there 2 options how DCG can be calculated (difference is in Penalty in divider):\n",
    "1. Standard - penalty of relevance = log(order)\n",
    "2. Industry - this method exponentially increases relevance, therefore if there are many values relevance of which is close to 1, in contrast with Standard method, this metric would adequately consider such output  \n",
    "![](./pic/3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Industry function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7344299716685585\n",
      "2.6189991399064203\n"
     ]
    }
   ],
   "source": [
    "# industry function:\n",
    "first_marks = [0.99, 0.95, 0.8, 0.98, 0.97]\n",
    "second_marks = [0.8, 0.99, 0.95, 0.98, 0.97]\n",
    "\n",
    "dcg_array_1 = []\n",
    "dcg_array_2 = []\n",
    "for order in range(1, len(first_marks)+1):\n",
    "    i = order - 1\n",
    "    relevance = first_marks[i]\n",
    "    dcg_i = (2**relevance - 1) / np.log2(order+1)\n",
    "    dcg_array_1.append(dcg_i)\n",
    "\n",
    "for order in range(1, len(second_marks)+1):\n",
    "    i = order - 1\n",
    "    relevance = second_marks[i]\n",
    "    dcg_i = (2**relevance - 1) / np.log2(order+1)\n",
    "    dcg_array_2.append(dcg_i)\n",
    "\n",
    "print(np.sum(dcg_array_1))\n",
    "print(np.sum(dcg_array_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.786693515822315\n",
      "2.6969307059651735\n"
     ]
    }
   ],
   "source": [
    "# standard function:\n",
    "first_marks = [0.99, 0.95, 0.8, 0.98, 0.97]\n",
    "second_marks = [0.8, 0.99, 0.95, 0.98, 0.97]\n",
    "\n",
    "dcg_array_1 = []\n",
    "dcg_array_2 = []\n",
    "for order in range(1, len(first_marks)+1):\n",
    "    i = order - 1\n",
    "    relevance = first_marks[i]\n",
    "    dcg_i = relevance / np.log2(order+1)\n",
    "    dcg_array_1.append(dcg_i)\n",
    "\n",
    "for order in range(1, len(second_marks)+1):\n",
    "    i = order - 1\n",
    "    relevance = second_marks[i]\n",
    "    dcg_i = relevance / np.log2(order+1)\n",
    "    dcg_array_2.append(dcg_i)\n",
    "\n",
    "print(np.sum(dcg_array_1))\n",
    "print(np.sum(dcg_array_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCG python function:\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def discounted_cumulative_gain(relevance: List[float], k: int, method: str = \"standard\") -> float:\n",
    "    \"\"\"Discounted Cumulative Gain\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    relevance : `List[float]`\n",
    "        Video relevance list\n",
    "    k : `int`\n",
    "        Count relevance to compute\n",
    "    method : `str`, optional\n",
    "        Metric implementation method, takes the values​​\n",
    "        `standard` - adds weight to the denominator\n",
    "        `industry` - adds weights to the numerator and denominator\n",
    "        `raise ValueError` - for any value\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : `float`\n",
    "        Metric score\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    relevance_considered = relevance[:k]\n",
    "    for order in range(1, len(relevance_considered)+1):\n",
    "        i = order - 1\n",
    "        if method == \"standard\":\n",
    "            dcg_i = relevance_considered[i] / np.log2(order+1)\n",
    "        else:\n",
    "            dcg_i = (2**relevance_considered[i] - 1) / np.log2(order+1)\n",
    "        score = score + dcg_i\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.786693515822315"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standard function:\n",
    "first_marks = [0.99, 0.95, 0.8, 0.98, 0.97]\n",
    "second_marks = [0.8, 0.99, 0.95, 0.98, 0.97]\n",
    "\n",
    "score = 0\n",
    "for order in range(1, len(first_marks)+1):\n",
    "    i = order - 1\n",
    "    relevance = first_marks[i]\n",
    "    dcg_i = relevance / np.log2(order+1)\n",
    "    score = score + dcg_i\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.786693515822315"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discounted_cumulative_gain(relevance = first_marks, k=5, method=\"standard\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalized Discounted Cumulative Gain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DCG@k - is not normalized metric, which makes task of comparing models with each other harder.  \n",
    "For example consider that we are comparing 2 models on the different queries:   \n",
    "in the first query, relevances can be close to 1 and in the second in contract to 0.   \n",
    "this may happen if in the search database there are documents relevant to the query, while there are no documents for the second query."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some times we need to average queries results, but we cannot add bananas to apples. That is why it would be better if they become of the same size (normalized) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ideal DCG:\n",
    "One of simple methods for normalization - divide on the maximum number.   \n",
    "What if we calculate DCG but documents would be sorted in advance in order by decreasing relevance? - It would be the maximum possible DCG for specific query with the specified number of documents considered (k) - IDCG (ideal discounted cumulative gain)  \n",
    "Now, to calculate nDCG (Normalized DCG) we can simply divide DCG on IDCG.  \n",
    "$$nDCG@k = \\frac{DCG}{IDCG}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcg = 2.6067348325982804\n",
      "dcg_ideal = 2.6164401144680056\n",
      "nDCG = 0.9962906539247512\n"
     ]
    }
   ],
   "source": [
    "relevances = [0.99, 0.94, 0.74, 0.88, 0.71, 0.68]\n",
    "relevances_sorted = list(np.sort(relevances)[::-1])\n",
    "k = 5\n",
    "method = 'standard'\n",
    "relevances_considered = relevances[:k]\n",
    "relevances_considered_sorted = relevances_sorted[:k]\n",
    "# dcg:\n",
    "dcg = discounted_cumulative_gain(relevances, k, method = method)\n",
    "print(f'dcg = {dcg}')\n",
    "# dcg_ideal:\n",
    "dcg_ideal = discounted_cumulative_gain(relevances_sorted, k, method = method)\n",
    "print(f'dcg_ideal = {dcg_ideal}')\n",
    "ndcg = dcg / dcg_ideal\n",
    "print(f'nDCG = {ndcg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function nDCG:\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "def discounted_cumulative_gain(relevance: List[float], k: int, method: str = \"standard\") -> float:\n",
    "    dcg = 0\n",
    "    relevance_considered = relevance[:k]\n",
    "    for order in range(1, len(relevance_considered)+1):\n",
    "        i = order - 1\n",
    "        if method == \"standard\":\n",
    "            dcg_i = relevance_considered[i] / np.log2(order+1)\n",
    "        else:\n",
    "            dcg_i = (2**relevance_considered[i] - 1) / np.log2(order+1)\n",
    "        dcg = dcg + dcg_i\n",
    "    return dcg\n",
    "\n",
    "def normalized_dcg(relevance: List[float], k: int, method: str = \"standard\") -> float:\n",
    "    \"\"\"Normalized Discounted Cumulative Gain.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    relevance : `List[float]`\n",
    "        Video relevance list\n",
    "    k : `int`\n",
    "        Count relevance to compute\n",
    "    method : `str`, optional\n",
    "        Metric implementation method, takes the values\n",
    "        `standard` - adds weight to the denominator\n",
    "        `industry` - adds weights to the numerator and denominator\n",
    "        `raise ValueError` - for any value\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : `float`\n",
    "        Metric score\n",
    "    \"\"\"\n",
    "    relevances_sorted = list(np.sort(relevance)[::-1])\n",
    "    # dcg:\n",
    "    dcg = discounted_cumulative_gain(relevance, k, method = method)\n",
    "    # dcg_ideal:\n",
    "    dcg_ideal = discounted_cumulative_gain(relevances_sorted, k, method = method)\n",
    "    score = dcg / dcg_ideal\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9962906539247512"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_dcg(relevance=relevances, k=k, method=method)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Normalized Discounted Cumulative Gain  \n",
    "Now, we can compare models by the specific query.  \n",
    "With growth of project, number of queries and documents significantly increases. Therefore, consider some specific queries does not make sense anymore."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we face the problem, that we need to consider high quantity of queries and monitor performance of the model in general. How we can calculate quality metrics for many queries?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Average nDCG** - average value of nDCG metric for each query in the list of queries.  \n",
    "$$(Average)nDCG = \\frac{sum(nDCG)(qi)}{n},$$  \n",
    "where qi - one query in the list, n - number of queries considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9961322104432755"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_relevances = [\n",
    "        [0.99, 0.94, 0.88, 0.89, 0.72, 0.65],\n",
    "        [0.99, 0.92, 0.93, 0.74, 0.61, 0.68], \n",
    "        [0.99, 0.96, 0.81, 0.73, 0.76, 0.69]\n",
    "    ]  \n",
    "k = 5\n",
    "method = 'standard'\n",
    "\n",
    "# calculation of Average nDCG:\n",
    "queries_number = len(list_relevances)\n",
    "score = 0\n",
    "for query in list_relevances:\n",
    "    nDCG_iter = normalized_dcg(query, k, method)\n",
    "    score = score + nDCG_iter\n",
    "score = score / queries_number\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average nDCG function:\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def discounted_cumulative_gain(relevance: List[float], k: int, method: str = \"standard\") -> float:\n",
    "    dcg = 0\n",
    "    relevance_considered = relevance[:k]\n",
    "    for order in range(1, len(relevance_considered)+1):\n",
    "        i = order - 1\n",
    "        if method == \"standard\":\n",
    "            dcg_i = relevance_considered[i] / np.log2(order+1)\n",
    "        else:\n",
    "            dcg_i = (2**relevance_considered[i] - 1) / np.log2(order+1)\n",
    "        dcg = dcg + dcg_i\n",
    "    return dcg\n",
    "\n",
    "def normalized_dcg(relevance: List[float], k: int, method: str = \"standard\") -> float:\n",
    "    relevances_sorted = list(np.sort(relevance)[::-1])\n",
    "    # dcg:\n",
    "    dcg = discounted_cumulative_gain(relevance, k, method = method)\n",
    "    # dcg_ideal:\n",
    "    dcg_ideal = discounted_cumulative_gain(relevances_sorted, k, method = method)\n",
    "    score = dcg / dcg_ideal\n",
    "    return score\n",
    "\n",
    "def avg_ndcg(list_relevances: List[List[float]], k: int, method: str = 'standard') -> float:\n",
    "    \"\"\"Average nDCG\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    list_relevances : `List[List[float]]`\n",
    "        Video relevance matrix for various queries\n",
    "    k : `int`\n",
    "        Count relevance to compute\n",
    "    method : `str`, optional\n",
    "        Metric implementation method, takes the values ​​\\\n",
    "        `standard` - adds weight to the denominator\\\n",
    "        `industry` - adds weights to the numerator and denominator\\\n",
    "        `raise ValueError` - for any value\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : `float`\n",
    "        Metric score\n",
    "    \"\"\"\n",
    "    queries_number = len(list_relevances)\n",
    "    score = 0\n",
    "    for query in list_relevances:\n",
    "        nDCG_iter = normalized_dcg(query, k, method)\n",
    "        score = score + nDCG_iter\n",
    "    score = score / queries_number\n",
    "\n",
    "    return score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the result we have Metrics that allows us to monitor general performance of different Ranging models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
