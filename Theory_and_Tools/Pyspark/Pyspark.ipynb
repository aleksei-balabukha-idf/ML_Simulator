{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark can be used for distributed computations in Python for data analysis and processing of Big Data and for ML"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark interacts with Spark though special library - Py4J. It allows python programs, which are executed by Interpreter, dynamically address to Java objects in JVM, translating code Scala in JVM. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark allows to conduct Parallel processing without need to use some Python modules for Flow or Multiprocessing work.   \n",
    "All complex communication and synchronization between flows, processes and even different CPU is processed in Spark."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:  \n",
    "Let's show simple example of PySpark usage.\n",
    "- Open CSV file,\n",
    "- Count number of rows\n",
    "- Show top 10 rows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark can not only connect to Spark cluster and process data there, but also can work with many different popular formats. It is very convenient for debugging scripts locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abal\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\sql\\context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 12\n",
      "root\n",
      " |-- column1;column2: string (nullable = true)\n",
      "\n",
      "+---------------+\n",
      "|column1;column2|\n",
      "+---------------+\n",
      "|            1;2|\n",
      "|            2;3|\n",
      "|            3;4|\n",
      "|            4;5|\n",
      "|            5;6|\n",
      "|            6;7|\n",
      "|            7;8|\n",
      "|            8;9|\n",
      "|           9;10|\n",
      "|          10;11|\n",
      "+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import PySpark libraries\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "# create a SparkSession:\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "\n",
    "# create a SQLContext:\n",
    "sqlContext = SQLContext(spark)\n",
    "\n",
    "#load a CSV file:\n",
    "df = sqlContext.read.csv(\"./data/example.csv\", header=True)\n",
    "\n",
    "# cound number of rows in the DataFrame\n",
    "rowCount = df.count()\n",
    "\n",
    "# print the row count:\n",
    "print(\"Number of rows:\", rowCount)\n",
    "\n",
    "# print the schema of the DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# show top 10 rows:\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
